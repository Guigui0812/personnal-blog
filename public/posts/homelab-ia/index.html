<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Héberger un modèle de langage (LLM) afin de bénéficier de la puissance de l’IA dans son homelab | grohee</title>
<meta name=keywords content><meta name=description content="Un LLM (Large Language Model) est un modèle d’intelligence artificielle (IA) entraîné sur de très grandes quantités de données, ce qui lui permet de comprendre et de générer du langage naturel. Concrètement, il peut résumer des documents, répondre à des questions variées, générer du code, ou agir sous forme d’agent en étant interfacé avec d’autres outils (terminal, navigateur ou des APIs).
Sans aborder précisément les aspects techniques, ces modèles fonctionnent en prédisant le mot suivant dans une phrase, en se basant sur le contexte des mots précédents. Le contexte des mots est représenté par un très grand nombre de paramètres aussi appelés poids (weights). Par exemple, GPT-3 de OpenAI possède 175 milliards de paramètres. Plus un modèle a de paramètres, plus il est performant dans la compréhension et la génération de texte. Il est ainsi capable de saisir une plus grande variété de contextes et de nuances dans le langage."><meta name=author content><link rel=canonical href=https://grohee.fr/posts/homelab-ia/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://grohee.fr/static/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://grohee.fr/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://grohee.fr/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://grohee.fr/assets/apple-touch-icon.png><link rel=mask-icon href=https://grohee.fr/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://grohee.fr/posts/homelab-ia/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://grohee.fr/posts/homelab-ia/"><meta property="og:site_name" content="grohee"><meta property="og:title" content="Héberger un modèle de langage (LLM) afin de bénéficier de la puissance de l’IA dans son homelab"><meta property="og:description" content="Un LLM (Large Language Model) est un modèle d’intelligence artificielle (IA) entraîné sur de très grandes quantités de données, ce qui lui permet de comprendre et de générer du langage naturel. Concrètement, il peut résumer des documents, répondre à des questions variées, générer du code, ou agir sous forme d’agent en étant interfacé avec d’autres outils (terminal, navigateur ou des APIs).
Sans aborder précisément les aspects techniques, ces modèles fonctionnent en prédisant le mot suivant dans une phrase, en se basant sur le contexte des mots précédents. Le contexte des mots est représenté par un très grand nombre de paramètres aussi appelés poids (weights). Par exemple, GPT-3 de OpenAI possède 175 milliards de paramètres. Plus un modèle a de paramètres, plus il est performant dans la compréhension et la génération de texte. Il est ainsi capable de saisir une plus grande variété de contextes et de nuances dans le langage."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-28T00:00:00+02:00"><meta property="article:modified_time" content="2025-09-28T00:00:00+02:00"><meta property="og:image" content="https://grohee.fr/images/ai-article-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://grohee.fr/images/ai-article-cover.png"><meta name=twitter:title content="Héberger un modèle de langage (LLM) afin de bénéficier de la puissance de l’IA dans son homelab"><meta name=twitter:description content="Un LLM (Large Language Model) est un modèle d’intelligence artificielle (IA) entraîné sur de très grandes quantités de données, ce qui lui permet de comprendre et de générer du langage naturel. Concrètement, il peut résumer des documents, répondre à des questions variées, générer du code, ou agir sous forme d’agent en étant interfacé avec d’autres outils (terminal, navigateur ou des APIs).
Sans aborder précisément les aspects techniques, ces modèles fonctionnent en prédisant le mot suivant dans une phrase, en se basant sur le contexte des mots précédents. Le contexte des mots est représenté par un très grand nombre de paramètres aussi appelés poids (weights). Par exemple, GPT-3 de OpenAI possède 175 milliards de paramètres. Plus un modèle a de paramètres, plus il est performant dans la compréhension et la génération de texte. Il est ainsi capable de saisir une plus grande variété de contextes et de nuances dans le langage."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://grohee.fr/posts/"},{"@type":"ListItem","position":2,"name":"Héberger un modèle de langage (LLM) afin de bénéficier de la puissance de l’IA dans son homelab","item":"https://grohee.fr/posts/homelab-ia/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Héberger un modèle de langage (LLM) afin de bénéficier de la puissance de l’IA dans son homelab","name":"Héberger un modèle de langage (LLM) afin de bénéficier de la puissance de l’IA dans son homelab","description":"Un LLM (Large Language Model) est un modèle d’intelligence artificielle (IA) entraîné sur de très grandes quantités de données, ce qui lui permet de comprendre et de générer du langage naturel. Concrètement, il peut résumer des documents, répondre à des questions variées, générer du code, ou agir sous forme d’agent en étant interfacé avec d’autres outils (terminal, navigateur ou des APIs).\nSans aborder précisément les aspects techniques, ces modèles fonctionnent en prédisant le mot suivant dans une phrase, en se basant sur le contexte des mots précédents. Le contexte des mots est représenté par un très grand nombre de paramètres aussi appelés poids (weights). Par exemple, GPT-3 de OpenAI possède 175 milliards de paramètres. Plus un modèle a de paramètres, plus il est performant dans la compréhension et la génération de texte. Il est ainsi capable de saisir une plus grande variété de contextes et de nuances dans le langage.\n","keywords":[],"articleBody":"Un LLM (Large Language Model) est un modèle d’intelligence artificielle (IA) entraîné sur de très grandes quantités de données, ce qui lui permet de comprendre et de générer du langage naturel. Concrètement, il peut résumer des documents, répondre à des questions variées, générer du code, ou agir sous forme d’agent en étant interfacé avec d’autres outils (terminal, navigateur ou des APIs).\nSans aborder précisément les aspects techniques, ces modèles fonctionnent en prédisant le mot suivant dans une phrase, en se basant sur le contexte des mots précédents. Le contexte des mots est représenté par un très grand nombre de paramètres aussi appelés poids (weights). Par exemple, GPT-3 de OpenAI possède 175 milliards de paramètres. Plus un modèle a de paramètres, plus il est performant dans la compréhension et la génération de texte. Il est ainsi capable de saisir une plus grande variété de contextes et de nuances dans le langage.\nBien que les LLMs nécessitent un volume de calcul important lors de leur entraînement, leur exécution nécessite bien moins de ressources. En effet, il est possible d’exécuter des modèles plus petits et optimisés sur du matériel grand public, notamment des cartes graphiques (GPU) telles que celles de NVIDIA ou AMD. Evidemment, pour exécuter les modèles les plus gourmands, il faut opter pour des GPU modernes et puissants, mais de nombreux modèles plus légers sont disponibles et peuvent fonctionner sur des cartes graphiques plus anciennes.\nUn LLM dans un homelab pour quels usages ? Exécuter un LLM localement dans son homelab présente des avantages à plusieurs égards.\nVie privée et sécurité Le premier avantage évident concerne la préservation de la vie privée. En hébergeant une IA localement, les données de l’utilisateur ne sont ni envoyées ni stockées à l’extérieur, ce qui réduit les risques de fuite de données sensibles, de surveillance ou d’usage commercial non désiré. Ces outils prenant en effet une place de plus en plus importante dans notre quotidien, et il peut arriver que des informations confidentielles soient partagées avec ces services.\nPersonnalisation et efficacité Un LLM local peut être personnalisé pour répondre à des besoins spécifiques. Sans entrer immédiatement dans les détails techniques, il est possible d’effectuer un fine-tuning du modèle en utilisant des données spécifiques à un contexte (documentation d’entreprise, FAQ…), ce qui permet d’améliorer la pertinence des réponses fournies par l’IA. De nouvelles technologies appelées RAG (Retrieval-Augmented Generation) permettent aussi de combiner un LLM avec une base de connaissances locale, pour des réponses encore plus précises.\nCoût Les coûts associés aux outils IA peuvent rapidement devenir importants : ChatGPT et Claude coûtent 24€ par mois, Gemini Pro est à 21€ par mois, et le plan Pro de Mistral est affiché à 17,99€ par mois. Ces tarifs, bien qu’attractifs, peuvent s’additionner rapidement si l’on souhaite bénéficier de plusieurs services en fonction des besoins : GPT pour la génération de texte et Claude pour la génération de code par exemple. L’avantage d’un LLM local est qu’il n’y a pas de coût récurrent, si ce n’est l’électricité consommée par le serveur et le coût initial du matériel. C’est même l’occasion de recycler du vieux matériel non utilisé puisque des modèles plus légers peuvent fonctionner sur d’anciens GPU.\nExpérimentation et liberté Comme mentionné précédemment, chaque LLM a ses forces et ses faiblesses. GPT est très performant pour générer du texte et Claude excelle dans la génération de code. En déployant une infrastructure permettant d’héberger des LLMs, on va pouvoir tester différents modèles, les comparer, et choisir celui qui convient le mieux à chaque tâche. L’utilisateur bénéficie même d’un choix plus large puisqu’il n’est pas limité à ceux proposant un chat en ligne. Il peut ainsi explorer des modèles open-source, parfois plus légers et adaptés à des usages spécifiques ou du matériel plus ancien.\nBénéficier de la puissance de l’IA Utiliser l’IA dans un homelab peut débloquer de nouveaux horizons d’automatisation et d’efficacité. Couplé à des outils comme n8n, Node-RED, ou des scripts personnalisés, un LLM peut permettre un traitement automatisé de tâches complexes, analyser des logs, fournir des rapports, envoyer des notifications personnalisées, effectuer de l’autoremediation, et bien plus encore. De plus, c’est un moyen intelligent d’explorer le monde de l’AIOps, c’est à dire l’IA appliquée au domaine des opérations IT (administration système, réseau, sécurité, etc.).\nRemarque : il est important de ne pas utiliser d’agent IA pouvant faire tout et n’importe quoi sur des serveurs. En effet, un agent mal configuré pourrait supprimer des fichiers importants, modifier des configurations, ou causer des interruptions de service. Il est donc crucial de définir des limites strictes et de surveiller les actions de l’agent. Cela peut être fait, par exemple, en créant un utilisateur dédié pour l’agent avec des permissions limitées sur les fichiers et les commandes qu’il peut exécuter (via les sudoers).\nMettre en place un LLM dans son homelab L’objet de ce guide est de détailler pas à pas l’installation d’un LLM sur une machine virtuelle (VM) Debian utilisant une carte graphique NVIDIA dans Proxmox. Le modèle fonctionnera avec Ollama, et une interface graphique sera fournie par Open WebUI, tous deux fonctionnant en tant que conteneurs Docker.\nOllama est une plateforme qui facilite la gestion et l’exécution locale de LLMs. Elle offre une API pour interagir avec les modèles, ainsi qu’une large bibliothèque de modèles pré-entraînés, dont un grand nombre sont open-source. Des modèles pour diverses applications sont disponibles : génération de texte, d’images, de code, etc.\nPrérequis :\nUn serveur hébergeant l’hyperviseur Proxmox VE Une carte graphique compatible NVIDIA (comme une GTX 970) Une image ISO de la distribution de votre choix (Debian 13 dans cet exemple) Remarque : les commandes à exécuter nécessitent des privilèges root, en utilisant sudo ou en se connectant directement en tant que root.\nCréation d’une VM Proxmox avec GPU Passthrough Le PCI Passthrough permet à une VM d’accéder directement à un périphérique PCI, dans notre cas il s’agit d’une carte graphique connectée directement à l’hyperviseur. Les modèles de langage nécessitant de grandes puissances de calculs pour fonctionner efficacement, même pour de la simple inférence, un GPU est donc indispensable pour assurer un fonctionnement optimal de ces outils. On peut évidemment faire du passthrough dans d’autres cas : carte réseau, carte graphique pour du jeu, de la 3D ou de la conversion vidéo (pour un serveur Plex ou Jellyfin par exemple).\nCette première étape nécessite des configuration Proxmox avancées, qui sont heureusement détaillées dans une section dédiée de la documentation de l’hyperviseur.\nVérifier que la fonctionnalité d’IOMMU est activée IOMMU (Input-Output Memory Management Unit) est une technologie qui permet de gérer la mémoire entre les périphériques et le système. Elle est indispensable au fonctionnement du PCI Passthrough car elle permet d’isoler les périphériques PCI, assurant donc que chaque VM puisse accéder directement au matériel qui lui est assigné, sans que d’autres systèmes n’interfèrent. Pour aller un peu plus loin, chaque périphérique se voit attribuer un groupe IOMMU.\nPour vérifier que cette fonctionnalité est bien activée, il faut exécuter la commande suivante dans le shell de Proxmox :\ndmesg | grep -e DMAR -e IOMMU Une ligne DMAR: IOMMU enabled devrait apparaître. Si ce n’est pas le cas, il faut activer l’IOMMU dans le BIOS de la machine hôte, puis ajouter les options suivantes au fichier /etc/default/grub :\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet intel_iommu=on iommu=pt\" # CPU Intel GRUB_CMDLINE_LINUX_DEFAULT=\"quiet amd_iommu=on iommu=pt\" # CPU AMD Une fois ces lignes ajoutées, il faut exécuter la commande update-grub2 et redémarrer le serveur pour que les modifications soient prises en compte.\nEnsuite, il faut vérifier que le remapping des interruptions IOMMU est bien activé :\ndmesg | grep 'remapping' Le résultat attendu est :\nAMD-Vi: Interrupt remapping enabled DMAR-IR: Enabled IRQ remapping in x2apic mode Si ce n’est pas le cas, il faudra ajouter la ligne suivante dans un fichier /etc/modprobe.d/iommu_unsafe_interrupts.conf :\noptions vfio_iommu_type1 allow_unsafe_interrupts=1 Enfin, il faut vérifier l’isolation IOMMU des périphériques PCI en effectuant pvesh get /nodes/{nodename}/hardware/pci --pci-class-blacklist \"\". Le résultat de cette commande est un tableau listant les périphériques PCI et leurs groupes IOMMU. Chaque groupe doit contenir un seul périphérique pour assurer un isolement correct.\nSi la commande ne fonctionne pas, il est possible que l’option ACS (Access Control Services) ne soit pas activée dans le BIOS. Cependant, si l’option n’est pas disponible, il faudra passer par l’utilisation du patch ACS de Alex Williamson. Le guide n’aborde pas cette étape puisqu’elle est optionnelle et dépend du matériel utilisé.\nConfigurer les modules VFIO VFIO (Virtual Function I/O) est un framework du noyau Linux qui permet aux utilisateurs d’accéder directement aux périphériques matériels depuis des environnements virtualisés. Il est utilisé pour le PCI Passthrough car il permet de sécuriser l’accès aux périphériques en isolant les ressources matérielles, garantissant ainsi que chaque VM puisse utiliser le matériel qui lui est assigné sans interférence.\nPlusieurs modules doivent être ajoutés au noyau pour pouvoir faire du PCI Passthrough. Pour cela, il faut ajouter les lignes suivantes dans le fichier /etc/modules :\nvfio vfio_iommu_type1 vfio_pci vfio_virqfd Pour appliquer ces modifications, il faut exécuter la commande update-initramfs et redémarrer le serveur.\nEmpêcher l’hyperviseur d’utiliser le GPU Avant de continuer, il faut empêcher l’hôte d’utiliser le GPU puisqu’il sera utilisé par une VM. Pour cela, il faut exclure les pilotes du GPU en ajoutant des exclusions dans un fichier /etc/modprobe.d/blacklist.conf :\nPour des GPU Nvidia : blacklist nouveau blacklist nvidia Pour des GPU AMD : blacklist amdgpu blacklist radeon Pour des GPU Intel : blacklist i915 Il est nécessaire de redémarrer le serveur pour appliquer ces modifications.\nCréer la VM avec le GPU passthrough Une fois les préconfigurations effectuées, il est temps de créer la VM qui accueillera le LLM.\nDans l’interface web de Proxmox, il faut créer une nouvelle VM qui aura la configuration suivante :\nType de machine : q35 BIOS : OVMF (UEFI) Affichage : VirtIO-GPU Une fois la VM créée, il faut ajouter un périphérique PCI dans l’onglet Hardware de la VM. La configuration doit être la suivante :\nLe reste de la configuration de la VM est à faire directement dans l’OS invité. Dans ce guide, on utilise Debian 13 mais d’autres distributions prennent en charge les GPU NVIDIA comme Ubuntu ou Fedora.\nConfigurer les drivers NVIDIA dans la VM L’ensemble de la procédure d’installation des drivers NVIDIA sur Debian est détaillée dans la documentation officielle de Debian. Cette procédure est essentielle afin que la VM puisse utiliser le GPU.\nTout d’abord, avant d’installer les drivers, il faut ajouter les dépôts contrib, non-free-firmware et non-free dans le fichier /etc/apt/sources.list afin de pouvoir télécharger les paquets nécessaires :\ndeb http://deb.debian.org/debian/ trixie main non-free-firmware non-free contrib deb-src http://deb.debian.org/debian/ trixie main non-free-firmware non-free contrib deb http://security.debian.org/debian-security trixie-security main non-free-firmware non-free contrib deb-src http://security.debian.org/debian-security trixie-security main non-free-firmware non-free contrib deb http://deb.debian.org/debian/ trixie-updates main non-free-firmware non-free contrib deb-src http://deb.debian.org/debian/ trixie-updates main non-free-firmware non-free contrib Par défaut, les dépôts contrib et non-free ne sont pas activés dans Debian. Ainsi, si on tente d’installer les drivers NVIDIA avec apt, le gestionnaire de paquets ne trouvera pas les drivers.\nAvant d’installer les drivers, il faut aussi installer le paquet linux-headers :\napt install linux-headers-amd64 # Choisir en fonction de l'architecture (amd64, arm64...) Lorsque les dépôts sont ajoutés, il faut mettre à jour la liste des paquets avec apt update puis installer l’utilitaire nvidia-detect qui va permettre de détecter la carte graphique et recommander les drivers appropriés :\napt install nvidia-detect nvidia-detect Cet utilitaire évite d’avoir à chercher manuellement quel driver est compatible avec la carte graphique.\nAvant l’installation, il faut ajouter une exclusion pour le driver nouveau dans un fichier /etc/modprobe.d/blacklist-nouveau.conf :\nblacklist nouveau options nouveau modeset=0 Cette exclusion est importante car elle évite que le driver nouveau soit chargé en même temps que celui que l’on installe, au risque de provoquer des conflits.\nIl reste enfin à installer les drivers recommandés par nvidia-detect :\napt install nvidia-kernel-dkms nvidia-driver firmware-misc-nonfree Une fois le redémarrage terminé, on vérifie l’installation avec la commande nvidia-smi ou dkms status :\nnvidia-smi dkms status Si la sortie de nvidia-smi affiche un tableau avec les informations du GPU, cela signifie que les drivers sont correctement installés et que le GPU peut être utilisé par la VM. Bravo !\nEn cas d’échec, plusieurs pistes peuvent être explorées. Il est possible que le Secure Boot soit activé dans le BIOS de la machine hôte. Dans ce cas, il faut le désactiver. Il peut aussi être intéressant de vérifier que le driver nouveau n’est pas installé avec la commande lsmod | grep nouveau. Si c’est le cas, il faut supprimer les paquets associés à ce driver.\nInstaller le NVIDIA Container Toolkit Puisque l’on souhaite utiliser ollama dans un conteneur Docker, il est nécessaire d’installer le NVIDIA Container Toolkit qui permet aux conteneurs d’utiliser le GPU.\nLa procédure abordée ici est issue de la documentation officielle de NVIDIA.\nDans un premier temps, il faut ajouter les dépôts NVIDIA :\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026\u0026 curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list Une fois ajoutés, il faut mettre à jour la liste des paquets avec sudo apt update puis installer le NVIDIA Container Toolkit :\nexport NVIDIA_CONTAINER_TOOLKIT_VERSION=1.17.8-1 sudo apt install -y \\ nvidia-container-toolkit=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ nvidia-container-toolkit-base=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container-tools=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container1=${NVIDIA_CONTAINER_TOOLKIT_VERSION} Ensuite, il faut configurer le runtime et redémarrer Docker pour que les modifications soient prises en compte :\nsudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker Enfin, on peut tester que le GPU est bien accessible depuis un conteneur Docker :\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi Si la sortie de la commande affiche un tableau avec les informations de la carte graphique, cela signifie que le NVIDIA Container Toolkit est correctement installé et que les conteneurs Docker peuvent utiliser le GPU. Ollama pourra dès lors bénéficier de l’accélération matérielle apportée par la carte afin d’exécuter des LLMs.\nOptionnel : corriger l’erreur Auto-detected mode as 'legacy' en créant une configuration CDI Si, lors de l’exécution du conteneur, l’erreur suivante apparaît, des manipulations supplémentaires sont nécessaires :\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running prestart hook #0: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy' Pour cela, il faut générer une configuration CDI (Container Device Interface). Cette configuration permet de définir comment les périphériques matériels, comme les GPU, sont exposés aux conteneurs. NVIDIA détaille cette procédure dans une section dédiée de sa documentation.\nDans un premier temps, il faut vérifier que les lignes suivantes apparaissent dans le fichier /etc/nvidia-container-runtime/config.toml :\n[nvidia-container-runtime-hook] path = \"nvidia-container-runtime-hook\" skip-mode-detection = true [nvidia-container-runtime] log-level = \"info\" mode = \"cdi\" runtimes = [\"docker-runc\", \"runc\", \"crun\"] [nvidia-container-runtime.modes.cdi] annotation-prefixes = [\"cdi.k8s.io/\"] default-kind = \"nvidia.com/gpu\" spec-dirs = [\"/etc/cdi\", \"/var/run/cdi\"] Si certaines lignes diffèrent, il faut les modifier en conséquence.\nUne fois cette modification effectuée, il faut redémarrer le service Docker avec systemctl restart docker puis relancer le conteneur. Cette étape terminée, on peut désormais créer une configuration CDI.\nPour cela, il faut éxécuter les commandes suivantes :\nnvidia-ctk cdi generate --output=/etc/cdi/cdi.yaml systemctl restart docker Afin de vérifier que la configuration a bien été générée, on peut utiliser la commande nvidia-ctk cdi list --load-specs.\nLa configuration générée doit faire apparaître les périphériques disponibles, par exemple :\n... devices: - containerEdits: deviceNodes: - path: /dev/nvidia0 name: \"0\" - containerEdits: deviceNodes: - path: /dev/nvidia0 name: GPU- - containerEdits: deviceNodes: - path: /dev/nvidia0 name: all kind: nvidia.com/gpu Ici, on voit que le périphérique /dev/nvidia0 est bien exposé dans le conteneur sous trois formes différentes : par son index 0, par son UUID GPU-xxxx, et sous le nom générique all.\nDès lors, on peut tester l’accès au GPU depuis un conteneur Docker avec la commande suivante :\ndocker run --rm --device 'nvidia.com/gpu=0' nvidia/cuda:12.4.1-base-ubuntu22.04 nvidia-smi Dans mon cas, j’ai été contraint d’effectuer ces manipulations car ma carte graphique GTX 970 n’est pas de première jeunesse. Sur une carte plus récente, il est possible que cette étape ne soit pas nécessaire.\nDésormais, la connexion entre le conteneur Docker et le GPU est fonctionnelle. Ollama peut être installé.\nDéployer Ollama et Open WebUI avec Docker Compose Pour installer Ollama et Open WebUI, il faut d’abord installer Docker et Docker Compose dans la VM. L’installation des outils est détaillée dans leurs documentations respectives :\nDocumentation officielle de Docker Documentation officielle de Docker Compose Le fichier docker compose suivant permet de déployer Ollama et Open WebUI très facilement :\nservices: ollama: image: ollama/ollama:latest container_name: ollama ports: - \"11434:11434\" volumes: - ollama:/root/.ollama environment: - OLLAMA_HOST=0.0.0.0 - OLLAMA_NUM_GPU=1 devices: - \"nvidia.com/gpu=0\" restart: unless-stopped openwebui: image: ghcr.io/open-webui/open-webui container_name: openwebui depends_on: [ ollama ] ports: - \"8080:8080\" environment: - OLLAMA_BASE_URL=http://ollama:11434 volumes: - openwebui:/app/backend/data restart: unless-stopped volumes: ollama: openwebui: Il ne reste qu’à exécuter la commande habituelle docker compose up -d pour lancer les services. Ollama sera accessible sur le port 11434, et Open WebUI sur le port 8080.\nUne fois les conteneurs démarrés, on peut vérifier que tout fonctionne correctement avec docker ps et se rendre à l’adresse http://:8080 pour accéder à l’interface web de Open WebUI.\nImporter un modèle dans Ollama Ollama ne possède pas de modèle par défaut, il faut importer ceux que l’on souhaite utiliser parmi une liste de modèles. Une fois choisi, on peut l’installer en utilisant la CLI fournie par l’outil.\nPour cela, on doit d’abord accéder au conteneur Ollama :\ndocker exec -it /bin/bash Une fois dans le conteneur, on peut exécuter un modèle parmi la liste susmentionnée :\nollama run gemma2:2b Une fois le modèle téléchargé, on peut l’utiliser directement en ligne de commande ou via l’interface web de Open WebUI.\nOn peut lister les LLMs installés avec la commande :\nollama list Il est également possible d’importer un modèle sans l’exécuter :\nollama pull gemma3 Pour trouver une large gamme de LLMs selon les cas d’usage, je recommande de se rendre sur Hugging Face qui référence une multitude de modèles open-source. Certains modèles disponibles sur Ollama y sont référencés, avec leurs caractéristiques techniques (taille, nombre de paramètres, type de licence…).\nExemple d’utilisation depuis l’interface web :\nL’exemple ci-dessus démontre avec succès l’utilisation du modèle gemma2:2b au sein de mon homelab personnel. On peut dès lors imaginer pléthore d’usages :\nAgents locaux pour effectuer des diagnostics et de l’autoremediation sur les serveurs du homelab Génération de rapports automatisés sur l’état des services Utilisation d’agents pour la vie quotidienne (assistant personnel, gestion de tâches…) sans que les données ne quittent le domicile Conclusion La mise en place d’un LLM auto-hébergé, au-delà de son aspect technique, ouvre la porte à une multitude d’usages. L’utilisation d’une IA locale garantit la confidentialité des données et permet d’expérimenter avec la grande variété de modèles disponibles. Couplée à des outils d’automatisation, comme n8n ou Node-RED, un homelab peut être transformé en un environnement intelligent et réactif, capable de gérer des tâches de manière autonome. Prochainement, je détaillerai comment j’ai utilisé mon IA locale pour automatiser certaines tâches en liant mes LLMs sur Ollama avec n8n.\nRéférences Self-Host a local AI platform! Ollama + Open WebUI - Christian Lempa - Youtube Ollama - GitHub Accélération matérielle GPU Passthrough sous Proxmox - Zogg NVIDIA Container Toolkit - Documentation PCI Passthrough - Proxmox VE Wiki Proxmox PCI(e) Passthrough in 2 minutes - Reddit NVIDIA Graphics Drivers - Debian Wiki Ollama Hugging Face - Models ","wordCount":"3215","inLanguage":"en","image":"https://grohee.fr/images/ai-article-cover.png","datePublished":"2025-09-28T00:00:00+02:00","dateModified":"2025-09-28T00:00:00+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://grohee.fr/posts/homelab-ia/"},"publisher":{"@type":"Organization","name":"grohee","logo":{"@type":"ImageObject","url":"https://grohee.fr/static/assets/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://grohee.fr/ accesskey=h title="grohee (Alt + H)">grohee</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://grohee.fr/categories/ title=Catégories><span>Catégories</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://grohee.fr/>Home</a>&nbsp;»&nbsp;<a href=https://grohee.fr/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Héberger un modèle de langage (LLM) afin de bénéficier de la puissance de l’IA dans son homelab</h1><div class=post-meta><span title='2025-09-28 00:00:00 +0200 CEST'>September 28, 2025</span>&nbsp;·&nbsp;3215 words</div></header><figure class=entry-cover><img loading=eager src=https://grohee.fr/images/ai-article-cover.png alt="AI dans un homelab avec Proxmox, Docker, Ollama et Open WebUI"><figcaption>AI dans un homelab avec Proxmox, Docker, Ollama et Open WebUI</figcaption></figure><div class=post-content><p>Un LLM (Large Language Model) est un modèle d’intelligence artificielle (IA) entraîné sur de très grandes quantités de données, ce qui lui permet de comprendre et de générer du langage naturel. Concrètement, il peut résumer des documents, répondre à des questions variées, générer du code, ou agir sous forme d’agent en étant interfacé avec d’autres outils (terminal, navigateur ou des APIs).</p><p>Sans aborder précisément les aspects techniques, ces modèles fonctionnent en prédisant le mot suivant dans une phrase, en se basant sur le contexte des mots précédents. Le contexte des mots est représenté par un très grand nombre de paramètres aussi appelés <strong>poids</strong> (weights). Par exemple, GPT-3 de OpenAI possède 175 milliards de paramètres. Plus un modèle a de paramètres, plus il est performant dans la compréhension et la génération de texte. Il est ainsi capable de saisir une plus grande variété de contextes et de nuances dans le langage.</p><p>Bien que les LLMs nécessitent un volume de calcul important lors de leur entraînement, leur exécution nécessite bien moins de ressources. En effet, il est possible d&rsquo;exécuter des modèles plus petits et optimisés sur du matériel grand public, notamment des cartes graphiques (GPU) telles que celles de NVIDIA ou AMD. Evidemment, pour exécuter les modèles les plus gourmands, il faut opter pour des GPU modernes et puissants, mais de nombreux modèles plus légers sont disponibles et peuvent fonctionner sur des cartes graphiques plus anciennes.</p><h2 id=un-llm-dans-un-homelab-pour-quels-usages->Un LLM dans un homelab pour quels usages ?<a hidden class=anchor aria-hidden=true href=#un-llm-dans-un-homelab-pour-quels-usages->#</a></h2><p>Exécuter un <strong>LLM</strong> localement dans son homelab présente des avantages à plusieurs égards.</p><h3 id=vie-privée-et-sécurité>Vie privée et sécurité<a hidden class=anchor aria-hidden=true href=#vie-privée-et-sécurité>#</a></h3><p>Le premier avantage évident concerne la préservation de la vie privée. En hébergeant une IA localement, les données de l&rsquo;utilisateur ne sont ni envoyées ni stockées à l&rsquo;extérieur, ce qui réduit les risques de fuite de données sensibles, de surveillance ou d&rsquo;usage commercial non désiré. Ces outils prenant en effet une place de plus en plus importante dans notre quotidien, et il peut arriver que des informations confidentielles soient partagées avec ces services.</p><h3 id=personnalisation-et-efficacité>Personnalisation et efficacité<a hidden class=anchor aria-hidden=true href=#personnalisation-et-efficacité>#</a></h3><p>Un LLM local peut être personnalisé pour répondre à des besoins spécifiques. Sans entrer immédiatement dans les détails techniques, il est possible d&rsquo;effectuer un <strong>fine-tuning</strong> du modèle en utilisant des données spécifiques à un contexte (documentation d&rsquo;entreprise, FAQ&mldr;), ce qui permet d&rsquo;améliorer la pertinence des réponses fournies par l&rsquo;IA. De nouvelles technologies appelées <strong>RAG (Retrieval-Augmented Generation)</strong> permettent aussi de combiner un LLM avec une base de connaissances locale, pour des réponses encore plus précises.</p><h3 id=coût>Coût<a hidden class=anchor aria-hidden=true href=#coût>#</a></h3><p>Les coûts associés aux outils <strong>IA</strong> peuvent rapidement devenir importants : <em>ChatGPT</em> et <em>Claude</em> coûtent 24€ par mois, <em>Gemini Pro</em> est à 21€ par mois, et le plan <em>Pro</em> de <em>Mistral</em> est affiché à 17,99€ par mois. Ces tarifs, bien qu&rsquo;attractifs, peuvent s&rsquo;additionner rapidement si l&rsquo;on souhaite bénéficier de plusieurs services en fonction des besoins : <em>GPT</em> pour la génération de texte et <em>Claude</em> pour la génération de code par exemple. L&rsquo;avantage d&rsquo;un LLM local est qu&rsquo;il n&rsquo;y a pas de coût récurrent, si ce n&rsquo;est l&rsquo;électricité consommée par le serveur et le coût initial du matériel. C&rsquo;est même l&rsquo;occasion de recycler du vieux matériel non utilisé puisque des modèles plus légers peuvent fonctionner sur d&rsquo;anciens GPU.</p><h3 id=expérimentation-et-liberté>Expérimentation et liberté<a hidden class=anchor aria-hidden=true href=#expérimentation-et-liberté>#</a></h3><p>Comme mentionné précédemment, chaque <strong>LLM</strong> a ses forces et ses faiblesses. <em>GPT</em> est très performant pour générer du texte et <em>Claude</em> excelle dans la génération de code. En déployant une infrastructure permettant d&rsquo;héberger des <strong>LLMs</strong>, on va pouvoir tester différents modèles, les comparer, et choisir celui qui convient le mieux à chaque tâche. L&rsquo;utilisateur bénéficie même d&rsquo;un choix plus large puisqu&rsquo;il n&rsquo;est pas limité à ceux proposant un chat en ligne. Il peut ainsi explorer des modèles open-source, parfois plus légers et adaptés à des usages spécifiques ou du matériel plus ancien.</p><h3 id=bénéficier-de-la-puissance-de-lia>Bénéficier de la puissance de l’IA<a hidden class=anchor aria-hidden=true href=#bénéficier-de-la-puissance-de-lia>#</a></h3><p>Utiliser l&rsquo;IA dans un homelab peut débloquer de nouveaux horizons d&rsquo;automatisation et d&rsquo;efficacité. Couplé à des outils comme <em>n8n</em>, <em>Node-RED</em>, ou des scripts personnalisés, un <strong>LLM</strong> peut permettre un traitement automatisé de tâches complexes, analyser des logs, fournir des rapports, envoyer des notifications personnalisées, effectuer de l&rsquo;autoremediation, et bien plus encore. De plus, c&rsquo;est un moyen intelligent d&rsquo;explorer le monde de <strong>l&rsquo;AIOps</strong>, c&rsquo;est à dire l&rsquo;IA appliquée au domaine des opérations IT (administration système, réseau, sécurité, etc.).</p><p><strong>Remarque</strong> : il est important de ne pas utiliser d&rsquo;agent IA pouvant faire tout et n&rsquo;importe quoi sur des serveurs. En effet, un agent mal configuré pourrait supprimer des fichiers importants, modifier des configurations, ou causer des interruptions de service. Il est donc crucial de définir des limites strictes et de surveiller les actions de l&rsquo;agent. Cela peut être fait, par exemple, en créant un utilisateur dédié pour l&rsquo;agent avec des permissions limitées sur les fichiers et les commandes qu&rsquo;il peut exécuter (via les <code>sudoers</code>).</p><h2 id=mettre-en-place-un-llm-dans-son-homelab>Mettre en place un LLM dans son homelab<a hidden class=anchor aria-hidden=true href=#mettre-en-place-un-llm-dans-son-homelab>#</a></h2><p>L&rsquo;objet de ce guide est de détailler pas à pas l&rsquo;installation d&rsquo;un <strong>LLM</strong> sur une machine virtuelle (VM) <em>Debian</em> utilisant une carte graphique <em>NVIDIA</em> dans <em>Proxmox</em>. Le modèle fonctionnera avec <em>Ollama</em>, et une interface graphique sera fournie par <em>Open WebUI</em>, tous deux fonctionnant en tant que conteneurs <em>Docker</em>.</p><p><em>Ollama</em> est une plateforme qui facilite la gestion et l&rsquo;exécution locale de <strong>LLMs</strong>. Elle offre une API pour interagir avec les modèles, ainsi qu&rsquo;une large bibliothèque de modèles pré-entraînés, dont un grand nombre sont open-source. Des modèles pour diverses applications sont disponibles : génération de texte, d&rsquo;images, de code, etc.</p><p><strong>Prérequis</strong> :</p><ul><li>Un serveur hébergeant l&rsquo;hyperviseur <em>Proxmox VE</em></li><li>Une carte graphique compatible <em>NVIDIA</em> (comme une <em>GTX 970</em>)</li><li>Une image ISO de la distribution de votre choix (Debian 13 dans cet exemple)</li></ul><p><strong>Remarque</strong> : les commandes à exécuter nécessitent des privilèges <em>root</em>, en utilisant <code>sudo</code> ou en se connectant directement en tant que <strong>root</strong>.</p><h3 id=création-dune-vm-proxmox-avec-gpu-passthrough>Création d&rsquo;une VM Proxmox avec GPU Passthrough<a hidden class=anchor aria-hidden=true href=#création-dune-vm-proxmox-avec-gpu-passthrough>#</a></h3><p>Le <strong>PCI Passthrough</strong> permet à une VM d&rsquo;accéder directement à un périphérique PCI, dans notre cas il s&rsquo;agit d&rsquo;une carte graphique connectée directement à l&rsquo;hyperviseur. Les modèles de langage nécessitant de grandes puissances de calculs pour fonctionner efficacement, même pour de la simple inférence, un GPU est donc indispensable pour assurer un fonctionnement optimal de ces outils. On peut évidemment faire du <strong>passthrough</strong> dans d&rsquo;autres cas : carte réseau, carte graphique pour du jeu, de la 3D ou de la conversion vidéo (pour un serveur <em>Plex</em> ou <em>Jellyfin</em> par exemple).</p><p>Cette première étape nécessite des configuration <em>Proxmox</em> avancées, qui sont heureusement détaillées dans une <a href=https://pve.proxmox.com/wiki/PCI_Passthrough>section dédiée de la documentation</a> de l&rsquo;hyperviseur.</p><h4 id=vérifier-que-la-fonctionnalité-diommu-est-activée>Vérifier que la fonctionnalité d&rsquo;IOMMU est activée<a hidden class=anchor aria-hidden=true href=#vérifier-que-la-fonctionnalité-diommu-est-activée>#</a></h4><p><strong>IOMMU</strong> (Input-Output Memory Management Unit) est une technologie qui permet de gérer la mémoire entre les périphériques et le système. Elle est indispensable au fonctionnement du <strong>PCI Passthrough</strong> car elle permet d&rsquo;isoler les périphériques PCI, assurant donc que chaque VM puisse accéder directement au matériel qui lui est assigné, sans que d&rsquo;autres systèmes n&rsquo;interfèrent. Pour aller un peu plus loin, chaque périphérique se voit attribuer un <strong>groupe IOMMU</strong>.</p><p>Pour vérifier que cette fonctionnalité est bien activée, il faut exécuter la commande suivante dans le shell de <em>Proxmox</em> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>dmesg | grep -e DMAR -e IOMMU
</span></span></code></pre></div><p>Une ligne <code>DMAR: IOMMU enabled</code> devrait apparaître. Si ce n&rsquo;est pas le cas, il faut activer l&rsquo;IOMMU dans le BIOS de la machine hôte, puis ajouter les options suivantes au fichier <code>/etc/default/grub</code> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>GRUB_CMDLINE_LINUX_DEFAULT<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;quiet intel_iommu=on iommu=pt&#34;</span> <span style=color:#75715e># CPU Intel</span>
</span></span><span style=display:flex><span>GRUB_CMDLINE_LINUX_DEFAULT<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;quiet amd_iommu=on iommu=pt&#34;</span> <span style=color:#75715e># CPU AMD</span>
</span></span></code></pre></div><p>Une fois ces lignes ajoutées, il faut exécuter la commande <code>update-grub2</code> et redémarrer le serveur pour que les modifications soient prises en compte.</p><p>Ensuite, il faut vérifier que le <strong>remapping des interruptions IOMMU</strong> est bien activé :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>dmesg | grep <span style=color:#e6db74>&#39;remapping&#39;</span>
</span></span></code></pre></div><p>Le résultat attendu est :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>AMD-Vi: Interrupt remapping enabled
</span></span><span style=display:flex><span>DMAR-IR: Enabled IRQ remapping in x2apic mode
</span></span></code></pre></div><p>Si ce n&rsquo;est pas le cas, il faudra ajouter la ligne suivante dans un fichier <code>/etc/modprobe.d/iommu_unsafe_interrupts.conf</code> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>options vfio_iommu_type1 allow_unsafe_interrupts<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>Enfin, il faut <strong>vérifier l&rsquo;isolation IOMMU des périphériques PCI</strong> en effectuant <code>pvesh get /nodes/{nodename}/hardware/pci --pci-class-blacklist ""</code>. Le résultat de cette commande est un tableau listant les périphériques PCI et leurs groupes IOMMU. Chaque groupe doit contenir un seul périphérique pour assurer un isolement correct.</p><p>Si la commande ne fonctionne pas, il est possible que l&rsquo;option <strong>ACS (Access Control Services)</strong> ne soit pas activée dans le BIOS. Cependant, si l&rsquo;option n&rsquo;est pas disponible, il faudra passer par l&rsquo;utilisation du <a href=https://lkml.org/lkml/2013/5/30/513>patch ACS de Alex Williamson</a>. Le guide n&rsquo;aborde pas cette étape puisqu&rsquo;elle est optionnelle et dépend du matériel utilisé.</p><h4 id=configurer-les-modules-vfio>Configurer les modules VFIO<a hidden class=anchor aria-hidden=true href=#configurer-les-modules-vfio>#</a></h4><p><strong>VFIO (Virtual Function I/O)</strong> est un framework du noyau Linux qui permet aux utilisateurs d&rsquo;accéder directement aux périphériques matériels depuis des environnements virtualisés. Il est utilisé pour le <strong>PCI Passthrough</strong> car il permet de sécuriser l&rsquo;accès aux périphériques en isolant les ressources matérielles, garantissant ainsi que chaque <strong>VM</strong> puisse utiliser le matériel qui lui est assigné sans interférence.</p><p>Plusieurs modules doivent être ajoutés au noyau pour pouvoir faire du <strong>PCI Passthrough</strong>. Pour cela, il faut ajouter les lignes suivantes dans le fichier <code>/etc/modules</code> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>vfio
</span></span><span style=display:flex><span>vfio_iommu_type1
</span></span><span style=display:flex><span>vfio_pci
</span></span><span style=display:flex><span>vfio_virqfd
</span></span></code></pre></div><p>Pour appliquer ces modifications, il faut exécuter la commande <code>update-initramfs</code> et redémarrer le serveur.</p><h4 id=empêcher-lhyperviseur-dutiliser-le-gpu>Empêcher l&rsquo;hyperviseur d&rsquo;utiliser le GPU<a hidden class=anchor aria-hidden=true href=#empêcher-lhyperviseur-dutiliser-le-gpu>#</a></h4><p>Avant de continuer, il faut empêcher l&rsquo;hôte d&rsquo;utiliser le GPU puisqu&rsquo;il sera utilisé par une VM. Pour cela, il faut exclure les pilotes du GPU en ajoutant des exclusions dans un fichier <code>/etc/modprobe.d/blacklist.conf</code> :</p><ul><li><strong>Pour des GPU Nvidia</strong> :</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>blacklist nouveau
</span></span><span style=display:flex><span>blacklist nvidia 
</span></span></code></pre></div><ul><li><strong>Pour des GPU AMD</strong> :</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>blacklist amdgpu
</span></span><span style=display:flex><span>blacklist radeon
</span></span></code></pre></div><ul><li><strong>Pour des GPU Intel</strong> :</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>blacklist i915
</span></span></code></pre></div><p>Il est nécessaire de redémarrer le serveur pour appliquer ces modifications.</p><h4 id=créer-la-vm-avec-le-gpu-passthrough>Créer la VM avec le GPU passthrough<a hidden class=anchor aria-hidden=true href=#créer-la-vm-avec-le-gpu-passthrough>#</a></h4><p>Une fois les préconfigurations effectuées, il est temps de créer la VM qui accueillera le <strong>LLM</strong>.</p><p>Dans l&rsquo;interface web de <em>Proxmox</em>, il faut créer une nouvelle VM qui aura la configuration suivante :</p><ul><li><strong>Type de machine</strong> : <code>q35</code></li><li><strong>BIOS</strong> : <code>OVMF (UEFI)</code></li><li><strong>Affichage</strong> : <code>VirtIO-GPU</code></li></ul><p>Une fois la VM créée, il faut ajouter un périphérique PCI dans l&rsquo;onglet <code>Hardware</code> de la VM. La configuration doit être la suivante :</p><p><img alt="Proxmox PCI passthrough settings" loading=lazy src=/images/proxmox-pci-passthrought.png></p><p>Le reste de la configuration de la VM est à faire directement dans l&rsquo;OS invité. Dans ce guide, on utilise <em>Debian 13</em> mais d&rsquo;autres distributions prennent en charge les GPU <em>NVIDIA</em> comme <em>Ubuntu</em> ou <em>Fedora</em>.</p><h4 id=configurer-les-drivers-nvidia-dans-la-vm>Configurer les drivers NVIDIA dans la VM<a hidden class=anchor aria-hidden=true href=#configurer-les-drivers-nvidia-dans-la-vm>#</a></h4><p>L&rsquo;ensemble de la procédure d&rsquo;installation des drivers <em>NVIDIA</em> sur <em>Debian</em> est détaillée dans la <a href=https://wiki.debian.org/NvidiaGraphicsDrivers>documentation officielle de Debian</a>. Cette procédure est essentielle afin que la VM puisse utiliser le GPU.</p><p>Tout d&rsquo;abord, avant d&rsquo;installer les drivers, il faut ajouter les dépôts <code>contrib</code>, <code>non-free-firmware</code> et <code>non-free</code> dans le fichier <code>/etc/apt/sources.list</code> afin de pouvoir télécharger les paquets nécessaires :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>deb http://deb.debian.org/debian/ trixie main non-free-firmware non-free contrib
</span></span><span style=display:flex><span>deb-src http://deb.debian.org/debian/ trixie main non-free-firmware non-free contrib
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>deb http://security.debian.org/debian-security trixie-security main non-free-firmware non-free contrib
</span></span><span style=display:flex><span>deb-src http://security.debian.org/debian-security trixie-security main non-free-firmware non-free contrib
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>deb http://deb.debian.org/debian/ trixie-updates main non-free-firmware non-free contrib
</span></span><span style=display:flex><span>deb-src http://deb.debian.org/debian/ trixie-updates main non-free-firmware non-free contrib
</span></span></code></pre></div><p>Par défaut, les dépôts <code>contrib</code> et <code>non-free</code> ne sont pas activés dans <em>Debian</em>. Ainsi, si on tente d&rsquo;installer les drivers <em>NVIDIA</em> avec <code>apt</code>, le gestionnaire de paquets ne trouvera pas les drivers.</p><p>Avant d&rsquo;installer les drivers, il faut aussi installer le paquet <code>linux-headers</code> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>apt install linux-headers-amd64 <span style=color:#75715e># Choisir en fonction de l&#39;architecture (amd64, arm64...)</span>
</span></span></code></pre></div><p>Lorsque les dépôts sont ajoutés, il faut mettre à jour la liste des paquets avec <code>apt update</code> puis installer l&rsquo;utilitaire <code>nvidia-detect</code> qui va permettre de détecter la carte graphique et recommander les drivers appropriés :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>apt install nvidia-detect
</span></span><span style=display:flex><span>nvidia-detect
</span></span></code></pre></div><p>Cet utilitaire évite d&rsquo;avoir à chercher manuellement quel driver est compatible avec la carte graphique.</p><p>Avant l&rsquo;installation, il faut ajouter une exclusion pour le driver <code>nouveau</code> dans un fichier <code>/etc/modprobe.d/blacklist-nouveau.conf</code> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>blacklist nouveau
</span></span><span style=display:flex><span>options nouveau modeset<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span></code></pre></div><p>Cette exclusion est importante car elle évite que le driver <code>nouveau</code> soit chargé en même temps que celui que l&rsquo;on installe, au risque de provoquer des conflits.</p><p>Il reste enfin à installer les drivers recommandés par <code>nvidia-detect</code> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>apt install nvidia-kernel-dkms nvidia-driver firmware-misc-nonfree 
</span></span></code></pre></div><p>Une fois le redémarrage terminé, on vérifie l&rsquo;installation avec la commande <code>nvidia-smi</code> ou <code>dkms status</code> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>nvidia-smi
</span></span><span style=display:flex><span>dkms status
</span></span></code></pre></div><p>Si la sortie de <code>nvidia-smi</code> affiche un tableau avec les informations du GPU, cela signifie que les drivers sont correctement installés et que le GPU peut être utilisé par la VM. Bravo !</p><p>En cas d&rsquo;échec, plusieurs pistes peuvent être explorées. Il est possible que le <strong>Secure Boot</strong> soit activé dans le BIOS de la machine hôte. Dans ce cas, il faut le désactiver. Il peut aussi être intéressant de vérifier que le driver <code>nouveau</code> n&rsquo;est pas installé avec la commande <code>lsmod | grep nouveau</code>. Si c&rsquo;est le cas, il faut supprimer les paquets associés à ce driver.</p><h3 id=installer-le-nvidia-container-toolkit>Installer le NVIDIA Container Toolkit<a hidden class=anchor aria-hidden=true href=#installer-le-nvidia-container-toolkit>#</a></h3><p>Puisque l&rsquo;on souhaite utiliser <em>ollama</em> dans un conteneur <em>Docker</em>, il est nécessaire d&rsquo;installer le <strong>NVIDIA Container Toolkit</strong> qui permet aux conteneurs d&rsquo;utiliser le GPU.</p><p>La procédure abordée ici est issue de la <a href=https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html>documentation officielle de NVIDIA</a>.</p><p>Dans un premier temps, il faut ajouter les dépôts <em>NVIDIA</em> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  <span style=color:#f92672>&amp;&amp;</span> curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    sed <span style=color:#e6db74>&#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39;</span> | <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
</span></span></code></pre></div><p>Une fois ajoutés, il faut mettre à jour la liste des paquets avec <code>sudo apt update</code> puis installer le <strong>NVIDIA Container Toolkit</strong> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export NVIDIA_CONTAINER_TOOLKIT_VERSION<span style=color:#f92672>=</span>1.17.8-1
</span></span><span style=display:flex><span>sudo apt install -y <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  nvidia-container-toolkit<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>NVIDIA_CONTAINER_TOOLKIT_VERSION<span style=color:#e6db74>}</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  nvidia-container-toolkit-base<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>NVIDIA_CONTAINER_TOOLKIT_VERSION<span style=color:#e6db74>}</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  libnvidia-container-tools<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>NVIDIA_CONTAINER_TOOLKIT_VERSION<span style=color:#e6db74>}</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  libnvidia-container1<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>NVIDIA_CONTAINER_TOOLKIT_VERSION<span style=color:#e6db74>}</span>
</span></span></code></pre></div><p>Ensuite, il faut configurer le runtime et redémarrer <em>Docker</em> pour que les modifications soient prises en compte :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo nvidia-ctk runtime configure --runtime<span style=color:#f92672>=</span>docker
</span></span><span style=display:flex><span>sudo systemctl restart docker
</span></span></code></pre></div><p>Enfin, on peut tester que le GPU est bien accessible depuis un conteneur <em>Docker</em> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo docker run --rm --runtime<span style=color:#f92672>=</span>nvidia --gpus all ubuntu nvidia-smi
</span></span></code></pre></div><p>Si la sortie de la commande affiche un tableau avec les informations de la carte graphique, cela signifie que le <strong>NVIDIA Container Toolkit</strong> est correctement installé et que les conteneurs <em>Docker</em> peuvent utiliser le GPU. <em>Ollama</em> pourra dès lors bénéficier de l&rsquo;accélération matérielle apportée par la carte afin d&rsquo;exécuter des <strong>LLMs</strong>.</p><h3 id=optionnel--corriger-lerreur-auto-detected-mode-as-legacy-en-créant-une-configuration-cdi>Optionnel : corriger l’erreur <code>Auto-detected mode as 'legacy'</code> en créant une configuration CDI<a hidden class=anchor aria-hidden=true href=#optionnel--corriger-lerreur-auto-detected-mode-as-legacy-en-créant-une-configuration-cdi>#</a></h3><p>Si, lors de l&rsquo;exécution du conteneur, l&rsquo;erreur suivante apparaît, des manipulations supplémentaires sont nécessaires :</p><pre tabindex=0><code>docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running prestart hook #0: exit status 1, stdout: , stderr: Auto-detected mode as &#39;legacy&#39;
</code></pre><p>Pour cela, il faut générer une configuration <strong>CDI (Container Device Interface)</strong>. Cette configuration permet de définir comment les périphériques matériels, comme les GPU, sont exposés aux conteneurs. <em>NVIDIA</em> détaille cette procédure dans une <a href=https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html>section dédiée de sa documentation</a>.</p><p>Dans un premier temps, il faut vérifier que les lignes suivantes apparaissent dans le fichier <code>/etc/nvidia-container-runtime/config.toml</code> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[<span style=color:#a6e22e>nvidia-container-runtime-hook</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>path</span> = <span style=color:#e6db74>&#34;nvidia-container-runtime-hook&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>skip-mode-detection</span> = <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#a6e22e>nvidia-container-runtime</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>log-level</span> = <span style=color:#e6db74>&#34;info&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>mode</span> = <span style=color:#e6db74>&#34;cdi&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>runtimes</span> = [<span style=color:#e6db74>&#34;docker-runc&#34;</span>, <span style=color:#e6db74>&#34;runc&#34;</span>, <span style=color:#e6db74>&#34;crun&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#a6e22e>nvidia-container-runtime</span>.<span style=color:#a6e22e>modes</span>.<span style=color:#a6e22e>cdi</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>annotation-prefixes</span> = [<span style=color:#e6db74>&#34;cdi.k8s.io/&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>default-kind</span> = <span style=color:#e6db74>&#34;nvidia.com/gpu&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>spec-dirs</span> = [<span style=color:#e6db74>&#34;/etc/cdi&#34;</span>, <span style=color:#e6db74>&#34;/var/run/cdi&#34;</span>]
</span></span></code></pre></div><p>Si certaines lignes diffèrent, il faut les modifier en conséquence.</p><p>Une fois cette modification effectuée, il faut redémarrer le service <em>Docker</em> avec <code>systemctl restart docker</code> puis relancer le conteneur. Cette étape terminée, on peut désormais créer une configuration <strong>CDI</strong>.</p><p>Pour cela, il faut éxécuter les commandes suivantes :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>nvidia-ctk cdi generate --output<span style=color:#f92672>=</span>/etc/cdi/cdi.yaml
</span></span><span style=display:flex><span>systemctl restart docker
</span></span></code></pre></div><p>Afin de vérifier que la configuration a bien été générée, on peut utiliser la commande <code>nvidia-ctk cdi list --load-specs</code>.</p><p>La configuration générée doit faire apparaître les périphériques disponibles, par exemple :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span><span style=color:#f92672>devices</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>containerEdits</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>deviceNodes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/dev/nvidia0</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#e6db74>&#34;0&#34;</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>containerEdits</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>deviceNodes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/dev/nvidia0</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>GPU-&lt;xxxx-xxxx-xxxx-xxxxxxxxxxxx&gt;</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>containerEdits</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>deviceNodes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/dev/nvidia0</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>all</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>nvidia.com/gpu</span>
</span></span></code></pre></div><p>Ici, on voit que le périphérique <code>/dev/nvidia0</code> est bien exposé dans le conteneur sous trois formes différentes : par son index <code>0</code>, par son UUID <code>GPU-xxxx</code>, et sous le nom générique <code>all</code>.</p><p>Dès lors, on peut tester l&rsquo;accès au GPU depuis un conteneur <em>Docker</em> avec la commande suivante :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --rm --device <span style=color:#e6db74>&#39;nvidia.com/gpu=0&#39;</span> nvidia/cuda:12.4.1-base-ubuntu22.04 nvidia-smi
</span></span></code></pre></div><p>Dans mon cas, j&rsquo;ai été contraint d&rsquo;effectuer ces manipulations car ma carte graphique <em>GTX 970</em> n&rsquo;est pas de première jeunesse. Sur une carte plus récente, il est possible que cette étape ne soit pas nécessaire.</p><p>Désormais, la connexion entre le conteneur <em>Docker</em> et le GPU est fonctionnelle. <em>Ollama</em> peut être installé.</p><h3 id=déployer-ollama-et-open-webui-avec-docker-compose>Déployer Ollama et Open WebUI avec Docker Compose<a hidden class=anchor aria-hidden=true href=#déployer-ollama-et-open-webui-avec-docker-compose>#</a></h3><p>Pour installer <em>Ollama</em> et <em>Open WebUI</em>, il faut d&rsquo;abord installer <em>Docker</em> et <em>Docker Compose</em> dans la VM. L&rsquo;installation des outils est détaillée dans leurs documentations respectives :</p><ul><li><a href=https://docs.docker.com/engine/install/debian/>Documentation officielle de Docker</a></li><li><a href=https://docs.docker.com/compose/install/>Documentation officielle de Docker Compose</a></li></ul><p>Le fichier <code>docker compose</code> suivant permet de déployer <em>Ollama</em> et <em>Open WebUI</em> très facilement :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>services</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>ollama</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>ollama/ollama:latest</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>container_name</span>: <span style=color:#ae81ff>ollama</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>      - <span style=color:#e6db74>&#34;11434:11434&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>ollama:/root/.ollama</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>OLLAMA_HOST=0.0.0.0</span>
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>OLLAMA_NUM_GPU=1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>devices</span>:
</span></span><span style=display:flex><span>      - <span style=color:#e6db74>&#34;nvidia.com/gpu=0&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>restart</span>: <span style=color:#ae81ff>unless-stopped</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>openwebui</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>ghcr.io/open-webui/open-webui</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>container_name</span>: <span style=color:#ae81ff>openwebui</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>depends_on</span>: [ <span style=color:#ae81ff>ollama ]</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>      - <span style=color:#e6db74>&#34;8080:8080&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>OLLAMA_BASE_URL=http://ollama:11434</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>openwebui:/app/backend/data </span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>restart</span>: <span style=color:#ae81ff>unless-stopped</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>ollama</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>openwebui</span>:
</span></span></code></pre></div><p>Il ne reste qu&rsquo;à exécuter la commande habituelle <code>docker compose up -d</code> pour lancer les services. <em>Ollama</em> sera accessible sur le port <code>11434</code>, et <em>Open WebUI</em> sur le port <code>8080</code>.</p><p>Une fois les conteneurs démarrés, on peut vérifier que tout fonctionne correctement avec <code>docker ps</code> et se rendre à l&rsquo;adresse <code>http://&lt;ip-de-la-vm>:8080</code> pour accéder à l&rsquo;interface web de <em>Open WebUI</em>.</p><h3 id=importer-un-modèle-dans-ollama>Importer un modèle dans Ollama<a hidden class=anchor aria-hidden=true href=#importer-un-modèle-dans-ollama>#</a></h3><p><em>Ollama</em> ne possède pas de modèle par défaut, il faut importer ceux que l&rsquo;on souhaite utiliser parmi une <a href=https://ollama.com/search>liste de modèles</a>. Une fois choisi, on peut l&rsquo;installer en utilisant la <strong>CLI</strong> fournie par l&rsquo;outil.</p><p>Pour cela, on doit d&rsquo;abord accéder au conteneur <em>Ollama</em> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker exec -it &lt;nom_du_conteneur&gt; /bin/bash
</span></span></code></pre></div><p>Une fois dans le conteneur, on peut exécuter un modèle parmi la liste susmentionnée :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ollama run gemma2:2b
</span></span></code></pre></div><p>Une fois le modèle téléchargé, on peut l&rsquo;utiliser directement en ligne de commande ou via l&rsquo;interface web de <em>Open WebUI</em>.</p><p>On peut lister les <strong>LLMs</strong> installés avec la commande :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ollama list
</span></span></code></pre></div><p>Il est également possible d&rsquo;importer un modèle sans l&rsquo;exécuter :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ollama pull gemma3
</span></span></code></pre></div><p>Pour trouver une large gamme de <strong>LLMs</strong> selon les cas d&rsquo;usage, je recommande de se rendre sur <a href=https://huggingface.co/models>Hugging Face</a> qui référence une multitude de modèles open-source. Certains modèles disponibles sur <em>Ollama</em> y sont référencés, avec leurs caractéristiques techniques (taille, nombre de paramètres, type de licence&mldr;).</p><p><strong>Exemple d&rsquo;utilisation depuis l&rsquo;interface web</strong> :</p><p><img alt="Open WebUI example" loading=lazy src=/images/openwebui-example.png></p><p>L&rsquo;exemple ci-dessus démontre avec succès l&rsquo;utilisation du modèle <em>gemma2:2b</em> au sein de mon homelab personnel. On peut dès lors imaginer pléthore d&rsquo;usages :</p><ul><li>Agents locaux pour effectuer des diagnostics et de l&rsquo;autoremediation sur les serveurs du homelab</li><li>Génération de rapports automatisés sur l&rsquo;état des services</li><li>Utilisation d&rsquo;agents pour la vie quotidienne (assistant personnel, gestion de tâches&mldr;) sans que les données ne quittent le domicile</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>La mise en place d&rsquo;un <strong>LLM</strong> auto-hébergé, au-delà de son aspect technique, ouvre la porte à une multitude d&rsquo;usages. L&rsquo;utilisation d&rsquo;une IA locale garantit la confidentialité des données et permet d&rsquo;expérimenter avec la grande variété de modèles disponibles. Couplée à des outils d&rsquo;automatisation, comme <em>n8n</em> ou <em>Node-RED</em>, <strong>un homelab peut être transformé en un environnement intelligent et réactif</strong>, capable de gérer des tâches de manière autonome. Prochainement, je détaillerai comment j&rsquo;ai utilisé mon IA locale pour automatiser certaines tâches en liant mes LLMs sur <em>Ollama</em> avec <em>n8n</em>.</p><h2 id=références>Références<a hidden class=anchor aria-hidden=true href=#références>#</a></h2><ul><li><a href="https://www.youtube.com/watch?v=RQFfK7xIL28">Self-Host a local AI platform! Ollama + Open WebUI - Christian Lempa - Youtube</a></li><li><a href=https://github.com/ollama/ollama>Ollama - GitHub</a></li><li><a href=https://infos.zogg.fr/pvegpupassthrough/>Accélération matérielle GPU Passthrough sous Proxmox - Zogg</a></li><li><a href=https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/index.html>NVIDIA Container Toolkit - Documentation</a></li><li><a href=https://pve.proxmox.com/wiki/PCI_Passthrough>PCI Passthrough - Proxmox VE Wiki</a></li><li><a href=https://www.reddit.com/r/Proxmox/comments/lcnn5w/proxmox_pcie_passthrough_in_2_minutes/>Proxmox PCI(e) Passthrough in 2 minutes - Reddit</a></li><li><a href=https://wiki.debian.org/NvidiaGraphicsDrivers>NVIDIA Graphics Drivers - Debian Wiki</a></li><li><a href=https://ollama.com/>Ollama</a></li><li><a href=https://huggingface.co/models>Hugging Face - Models</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://grohee.fr/>grohee</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>